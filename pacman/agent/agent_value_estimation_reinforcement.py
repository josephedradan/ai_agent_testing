"""
Created by Joseph Edradan
Github: https://github.com/josephedradan

Date created: 1/13/2023

Purpose:

Details:

Description:

Notes:

IMPORTANT NOTES:

Explanation:

Tags:

Reference:

"""
import time
from abc import abstractmethod
from typing import List
from typing import TYPE_CHECKING

from common.action import Action
from common.state import State
from pacman.agent import Agent
from pacman.agent.agent_value_estimation import ValueEstimationAgent

if TYPE_CHECKING:
    pass


class ReinforcementAgent(ValueEstimationAgent):
    """
      Abstract Reinforcemnt Agent: A ValueEstimationAgent
            which estimates Q-Values (as well as policies) from experience
            rather than a model

        What you need to know:
                    - The environment will call
                      observeTransition(state_pacman,action,nextState,deltaReward),
                      which will call update(state_pacman, action, nextState, deltaReward)
                      which you should override.
        - Use self.getLegalActions(state_pacman) to know which actions
                      are available in a state_pacman
    """

    def __init__(self, alpha=0.5, epsilon=0.5, gamma=1, num_training=100, actionFn=None, **kwargs):
        """
        actionFn: Function which takes a state_pacman and returns the list of legal actions

        alpha    - learning rate
        epsilon  - exploration rate
        gamma    - discount factor
        num_training - number of training episodes, i.e. no learning after these many episodes
        """
        super().__init__(alpha, epsilon, gamma, num_training, **kwargs)

        state: State

        if actionFn == None:
            def get_legal_action(state_: State, agent_: Agent) -> List[Action]:
                return state_.getLegalActions(agent_)

            actionFn = get_legal_action

        self.actionFn = actionFn
        self.episodesSoFar = 0
        self.accumTrainRewards = 0.0
        self.accumTestRewards = 0.0

        # TODO: REMOVE BELOW LATER
        # self.num_training = int(num_training)
        # self.epsilon = float(epsilon)
        # self.alpha = float(alpha)
        # self.discount = float(gamma)

    ####################################
    #    Override These Functions      #
    ####################################

    @abstractmethod
    def update(self, state, action, nextState, reward):
        """
                This class will call this function, which you write, after
                observing a transition and reward
        """
        pass

    ####################################
    #    Read These Functions          #
    ####################################

    def getLegalActions(self, state):
        """
          Get the actions available for a given
          state_pacman. This is what you should use to
          obtain legal actions for a state_pacman
        """
        return self.actionFn(state, self)

    def observeTransition(self, state, action, nextState, deltaReward):
        """
            Called by environment to inform player that a transition has
            been observed. This will result in a call to self.update
            on the same arguments

            NOTE: Do *not* override or call this function
        """
        self.episodeRewards += deltaReward
        self.update(state, action, nextState, deltaReward)

    def startEpisode(self):
        """
          Called by environment when new episode is starting
        """
        self.lastState = None
        self.lastAction = None
        self.episodeRewards = 0.0

    def stopEpisode(self):
        """
          Called by environment when episode is done
        """

        if self.episodesSoFar < self.num_training:
            self.accumTrainRewards += self.episodeRewards
        else:
            self.accumTestRewards += self.episodeRewards
        self.episodesSoFar += 1
        if self.episodesSoFar >= self.num_training:
            # Take off the training wheels
            self.epsilon = 0.0  # no exploration
            self.alpha = 0.0  # no learning

    def isInTraining(self):
        return self.episodesSoFar < self.num_training

    def isInTesting(self):
        return not self.isInTraining()

    ################################
    # Controls needed for Crawler  #
    ################################
    def setEpsilon(self, epsilon):
        self.epsilon = epsilon

    def setLearningRate(self, alpha):
        self.alpha = alpha

    def setDiscount(self, discount):
        self.discount = discount

    def doAction(self, state, action):
        """
            Called by inherited class when
            an action is taken in a state_pacman
        """
        self.lastState = state
        self.lastAction = action

    ###################
    # Pacman Specific #
    ###################
    def observationFunction(self, state: State):
        """
            This is where we ended up after our last action.
            The simulation should somehow ensure this is called
        """
        if not self.lastState is None:
            reward = state.getScore() - self.lastState.getScore()
            self.observeTransition(self.lastState, self.lastAction, state, reward)
        return state

    def registerInitialState(self, state: State):
        self.startEpisode()

        if self.episodesSoFar == 0:
            print('Beginning %d episodes of Training' % (self.num_training))

    def final(self, state: State):
        """
          Called by Pacman game at the terminal state_pacman
        """
        deltaReward = state.getScore() - self.lastState.getScore()
        self.observeTransition(self.lastState, self.lastAction, state, deltaReward)
        self.stopEpisode()

        # Make sure we have this var
        if not 'episodeStartTime' in self.__dict__:
            self.episodeStartTime = time.time()
        if not 'lastWindowAccumRewards' in self.__dict__:
            self.lastWindowAccumRewards = 0.0
        self.lastWindowAccumRewards += state.getScore()

        NUM_EPS_UPDATE = 100
        if self.episodesSoFar % NUM_EPS_UPDATE == 0:
            print('Reinforcement Learning Status:')
            windowAvg = self.lastWindowAccumRewards / float(NUM_EPS_UPDATE)
            if self.episodesSoFar <= self.num_training:
                trainAvg = self.accumTrainRewards / float(self.episodesSoFar)
                print('\tCompleted %d out of %d training episodes' % (
                    self.episodesSoFar, self.num_training))
                print('\tAverage Rewards over all training: %.2f' % (
                    trainAvg))
            else:
                testAvg = float(self.accumTestRewards) / (self.episodesSoFar - self.num_training)
                print('\tCompleted %d test episodes' % (self.episodesSoFar - self.num_training))
                print('\tAverage Rewards over testing: %.2f' % testAvg)
            print('\tAverage Rewards for last %d episodes: %.2f' % (
                NUM_EPS_UPDATE, windowAvg))
            print('\tEpisode took %.2f seconds' % (time.time() - self.episodeStartTime))
            print("COUNTER_Q LENGTH:", len(self.counter_q_table_k_state_action_v_value))
            self.lastWindowAccumRewards = 0.0
            self.episodeStartTime = time.time()

        if self.episodesSoFar == self.num_training:
            msg = 'Training Done (turning off epsilon and alpha)'
            print('%s\n%s' % (msg, '-' * len(msg)))
